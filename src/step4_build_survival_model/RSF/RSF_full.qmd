---
title: 'Build Survival Model: Random Survival Forest'
author: 'Mingcheng Hu'
params:
    adjust_type:
        label: 'Adjustment Type of Survival Model'
        value: 'full'
        choices:
            - 'minimal'
            - 'partial'
            - 'full'
    impute_type:
        label: 'Imputation Type'
        value: 'imputed'
        choices:
            - 'unimputed'
            - 'imputed'
    include_statin:
        label: 'Include Participants Taking Statin in the Model'
        value: 'no'
        choices:
            - 'yes'
            - 'no'
format: 
    pdf:
        toc: true
        keep-tex: true
        include-in-header: 
            text: |
                \usepackage{fvextra}
                \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
                \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r output = FALSE}
library(tidyverse)
library(survival)
library(randomForestSRC)
library(caret)
library(survcomp)
library(parallel)
library(doParallel)
library(mcprogress) # wrap mclapply with progress bar.
library(kableExtra) # include knitr automatically

source("/work/users/y/u/yuukias/BIOS-Material/BIOS992/utils/csv_utils.r")
# * Don't use setwd() for Quarto documents!
# setwd("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data")

adjust_type <- ifelse(exists("params"), params$adjust_type, "full") # options: "minimal", "partial", "full"
impute_type <- ifelse(exists("params"), params$impute_type, "imputed") # options: "unimputed", "imputed"
include_statin <- ifelse(exists("params"), params$include_statin, "no") # options: "yes", "no"

n_folds <- 10
set.seed(1234)
```

```{r}
# string of parameters
adjust_type_str <- switch(adjust_type,
    minimal = "minimal",
    partial = "partial",
    full = "full"
)
print(paste0("Model Adjustment Type: ", adjust_type_str))
impute_type_str <- switch(impute_type,
    unimputed = "unimputed",
    imputed = "imputed"
)
print(paste0("Data Imputation Type: ", impute_type_str))
```

# Load Data

```{r}
if (include_statin == "yes") {
    data_train <- read.csv(paste0("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data/train_data_", impute_type_str, "_statin.csv"),
        header = TRUE
    )
} else {
    data_train <- read.csv(paste0("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data/train_data_", impute_type_str, ".csv"),
        header = TRUE
    )
}

data_train <- data_train[, -1] # the first column is the index generated by sklearn
(dim(data_train))
```

```{r}
data <- select_subset(data_train, type = adjust_type)
(dim(data))
```

```{r}
colnames(data)
```

```{r}
data <- tibble::as_tibble(data)
```

```{r label = "Full Data Specific Processing"}
# * There are some imputed ethnicity set to "e". We will exclude them at this time.
data <- data %>%
    filter(ethnicity != "e")

# * We also need to manually relevel the categorical variables
data <- data %>%
    mutate(
        # Set "Never" (0) as baseline for smoking
        smoking = factor(smoking,
            levels = c("0", "1", "2", "-3"),
            labels = c("Never", "Previous", "Current", "Prefer not to answer")
        ),

        # Set "No" (0) as baseline for diabetes
        diabetes = factor(diabetes,
            levels = c("0", "1", "-1", "-3"),
            labels = c("No", "Yes", "Do not know", "Prefer not to answer")
        ),

        # Ensure other categorical variables are properly factored
        ethnicity = factor(ethnicity,
            levels = c("1", "2", "3", "4", "5", "6"),
            labels = c("White", "Mixed", "Asian/Asian British", "Black/Black British", "Chinese", "Other")
        ),
        education = factor(education,
            levels = c("1", "2", "3", "4", "5", "6", "-7", "-3"),
            labels = c(
                "College/University degree", "A levels/AS levels",
                "O levels/GCSEs", "CSEs", "NVQ/HND/HNC",
                "Other professional", "None of the above",
                "Prefer not to answer"
            )
        ),
        activity = factor(activity,
            levels = c("0", "1", "2"),
            labels = c("Low", "Moderate", "High")
        ),
        sex = factor(sex,
            levels = c("0", "1"),
            labels = c("Female", "Male")
        ),
        hypertension_treatment = factor(hypertension_treatment,
            levels = c("0", "1"),
            labels = c("No", "Yes")
        )
    )
```

```{r}
# * It is very hard to compare the HR as different predictors are on different magnitudes, so we need to normalize them.
time_col <- data$time
event_col <- data$event
data <- data %>%
    select(-c(time, event)) %>%
    mutate(across(where(is.numeric), scale)) %>%
    mutate(
        time = time_col,
        event = event_col
    )
```

**Note now the interpretation of HR is different! For example, if HR=1.16 for the predictor in the univariate model fitted using scaled data, it means that each standard deviation increase is associated with 16% higher risk of event.**

```{r}
# For RSF model, we don't need to exclude the missing values
```

# Random Survival Forest (RSF)

## Variable Selection

The `method` argument can be set to `vh` instead for variable hunting, which should be used for problems where the number of variables is substantially larger than the sample size.

```{r}
n_cores <- min(parallel::detectCores() - 1, 32)
cl <- makeCluster(n_cores)
registerDoParallel(cl)
```

```{r label = "Variable Selection", eval = FALSE}
rsf_var_select <- var.select.rfsrc(Surv(time, event) ~ .,
    data = data,
    method = "md",
    seed = 1234,
    ntree = 200,
    parallel = TRUE
) # minimal depth variable selection

stopCluster(cl)
```

```{r echo = FALSE, eval = FALSE}
save(rsf_var_select,
    file = get_data_path("rsf_var_select_model", adjust_type_str, impute_type_str, include_statin, model = "rsf")
)
```

```{r echo = FALSE}
load(get_data_path("rsf_var_select_model", adjust_type_str, impute_type_str, include_statin, model = "rsf"))
```

```{r}
vars_ranked <- rsf_var_select$topvars
```

## Cross Validation to Select the Best Number of Features

We will use 10-fold cross validation to select the best number of features used in the model.

```{r label = "Cross Validation to Select the Best Number of Features", eval = FALSE}
set.seed(1234)
folds <- createFolds(data$event, k = n_folds) # return indices of folds

cv_errors <- pmclapply(seq(1, length(vars_ranked), by = 1), function(num_vars) {
    message(paste0("Calculating the CV error with ", num_vars, " variables"))
    selected_vars <- vars_ranked[1:num_vars]

    fold_errors <- sapply(folds, function(fold_idx) {
        # * We adopt same approach as XGBoost to avoid segmentation fault.
        train_data_fold <- data[-fold_idx, c("time", "event", selected_vars)]
        train_data_fold <- model.frame(~ . - 1, data = train_data_fold, na.action = na.pass)
        train_data_fold <- model.matrix(~ . - 1, data = train_data_fold)
        train_data_fold <- as.data.frame(train_data_fold)
        val_data_fold <- data[fold_idx, c("time", "event", selected_vars)]
        val_data_fold <- model.frame(~ . - 1, data = val_data_fold, na.action = na.pass)
        val_data_fold <- model.matrix(~ . - 1, data = val_data_fold)
        val_data_fold <- as.data.frame(val_data_fold)
        model <- rfsrc.fast(Surv(time, event) ~ .,
            data = train_data_fold,
            ntree = 200,
            forest = TRUE
        )
        pred <- predict(model,
            newdata = val_data_fold,
            na.action = "na.impute"  # * There may be missing values in the dataset
        )$predicted  # define pred has attributes survival(sample_size*time) and predicted(sample_size) for risk
        # Use C-index to measure the performance of the model
        1 - concordance.index(
            pred,  # pass risk prediction for first argument
            val_data_fold$time, 
            val_data_fold$event
        )$c.index
    })
    mean(fold_errors)
}, title = "Cross Validation to Select the Best Number of Features")
```

```{r include = FALSE, eval = FALSE}
best_num_vars <- which.min(cv_errors)
vars_selected <- vars_ranked[1:best_num_vars]
```

```{r echo = FALSE, eval = FALSE}
save(cv_errors, vars_selected,
    file = get_data_path("rsf_var_select_name", adjust_type_str, impute_type_str, include_statin, model = "rsf")
)
```

```{r echo = FALSE}
load(get_data_path("rsf_var_select_name", adjust_type_str, impute_type_str, include_statin, model = "rsf"))
```

```{r}
cv_errors <- as.numeric(cv_errors)
plot(1:length(cv_errors), cv_errors)
best_num_vars <- which.min(cv_errors)
vars_selected <- vars_ranked[1:best_num_vars]
```

```{r}
print(paste0("The best number of features to retain is ", best_num_vars))
```

## Model Fitting

```{r label = "Model Fitting"}
data_selected <- data[, c("time", "event", vars_selected)]
data_selected <- model.frame(~ . - 1, data = data_selected, na.action = na.pass)
data_selected <- model.matrix(~ . - 1, data = data_selected)
data_selected <- as.data.frame(data_selected)

# Before formally fitting the model, we can tune the hyperparameters to find:
# 1. optimal mtry (possible split at each node)
# 2. optimal nodesize (minimum size of terminal nodes)
rsf_tuned <- tune.rfsrc(
    Surv(time, event) ~ .,
    data = data_selected,
)

rsf_model <- rfsrc(Surv(time, event) ~ .,
    data = data_selected,
    ntree = 500,
    mtry = rsf_tuned$best.mtry,
    nodesize = rsf_tuned$best.nodesize
)
```

```{r}
data_full <- model.frame(~ . - 1, data = data, na.action = na.pass)
data_full <- model.matrix(~ . - 1, data = data_full)
data_full <- as.data.frame(data_full)
# We also fit the full model
rsf_tuned_full <- tune.rfsrc(
    Surv(time, event) ~ .,
    data = data_full,
)

rsf_model_full <- rfsrc(Surv(time, event) ~ .,
    data = data_full,
    ntree = 1000,
    mtry = rsf_tuned_full$best.mtry,
    nodesize = rsf_tuned_full$best.nodesize
)
```

```{r echo = FALSE}
save(rsf_model, rsf_model_full,
    file = get_data_path("rsf_model", adjust_type_str, impute_type_str, include_statin, model = "rsf")
)
```

```{r echo = FALSE}
load(get_data_path("rsf_model", adjust_type_str, impute_type_str, include_statin, model = "rsf"))
```

