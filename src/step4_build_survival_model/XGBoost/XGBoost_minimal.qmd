---
title: 'Build Survival Model: XGBoost'
author: 'Mingcheng Hu'
params:
    adjust_type:
        label: 'Adjustment Type of Survival Model'
        value: 'minimal'
        choices:
            - 'minimal'
            - 'partial'
            - 'full'
    impute_type:
        label: 'Imputation Type'
        value: 'unimputed'
        choices:
            - 'unimputed'
            - 'imputed'
    include_statin:
        label: 'Include Participants Taking Statin in the Model'
        value: 'no'
        choices:
            - 'yes'
            - 'no'
format: 
    pdf:
        toc: true
        keep-tex: true
        include-in-header: 
            text: |
                \usepackage{fvextra}
                \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
                \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r output = FALSE}
library(tidyverse)
library(survival)
library(xgboost)
library(caret)
library(survcomp)
library(parallel)
library(mcprogress) # wrap mclapply with progress bar.
library(kableExtra) # include knitr automatically
library(mlr3)  # hyperparameter tuning
library(mlr3tuning)
library(paradox)

source("/work/users/y/u/yuukias/BIOS-Material/BIOS992/utils/csv_utils.r")
# * Don't use setwd() for Quarto documents!
# setwd("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data")

adjust_type <- ifelse(exists("params"), params$adjust_type, "minimal") # options: "minimal", "partial", "full"
impute_type <- ifelse(exists("params"), params$impute_type, "unimputed") # options: "unimputed", "imputed"
include_statin <- ifelse(exists("params"), params$include_statin, "no") # options: "yes", "no"

# hyperparameter tuning trials
n_trials <- 50   # * It is recommended to set it to n_params * (8~10)
n_folds <- 10
set.seed(1234)
```

```{r}
# string of parameters
adjust_type_str <- switch(adjust_type,
    minimal = "minimal",
    partial = "partial",
    full = "full"
)
print(paste0("Model Adjustment Type: ", adjust_type_str))
impute_type_str <- switch(impute_type,
    unimputed = "unimputed",
    imputed = "imputed"
)
print(paste0("Data Imputation Type: ", impute_type_str))
```

# Load Data

```{r}
if (include_statin == "yes") {
    data_train <- read.csv(paste0("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data/train_data_", impute_type_str, "_statin.csv"),
        header = TRUE
    )
} else {
    data_train <- read.csv(paste0("/work/users/y/u/yuukias/BIOS-Material/BIOS992/data/train_data_", impute_type_str, ".csv"),
        header = TRUE
    )
}

data_train <- data_train[, -1] # the first column is the index generated by sklearn
(dim(data_train))
```

```{r}
data <- select_subset(data_train, type = adjust_type)
(dim(data))
```

```{r}
colnames(data)
```

```{r}
data <- tibble::as_tibble(data)
```

```{r}
# * It is very hard to compare the HR as different predictors are on different magnitudes, so we need to normalize them.
time_col <- data$time
event_col <- data$event
data <- data %>%
    select(-c(time, event)) %>%
    mutate(across(where(is.numeric), scale)) %>%
    mutate(
        time = time_col,
        event = event_col
    )
```

**Note now the interpretation of HR is different! For example, if HR=1.16 for the predictor in the univariate model fitted using scaled data, it means that each standard deviation increase is associated with 16% higher risk of event.**

```{r}
# For XGBoost model, we create a validation set for early stopping.
set.seed(1234)
train_index <- createDataPartition(
    data$event, # stratify by event
    p = 0.8,
    list = FALSE
)

train_data <- data[train_index, ]
val_data <- data[-train_index, ]
```

# XGBoost

## Data Preparation

```{r}
total_x <- as.matrix(data %>% select(-c(time, event)))
total_y <- data %>% select(time, event)
# * Note format of label should be different when using Cox model and AFT model.
# define For uncensored labels, use [a,a]
# define For right-censored labels, use [a,Inf]
total_y_lower_bound <- data$time
total_y_upper_bound <- ifelse(data$event == 1, data$time, Inf)

train_x <- as.matrix(train_data %>% select(-c(time, event)))
train_y_lower_bound <- train_data$time
train_y_upper_bound <- ifelse(train_data$event == 1, train_data$time, Inf)
dtrain <- xgb.DMatrix(
    data = train_x,
    label_lower_bound = train_y_lower_bound,
    label_upper_bound = train_y_upper_bound
)

val_x <- as.matrix(val_data %>% select(-c(time, event)))
val_y_lower_bound <- val_data$time
val_y_upper_bound <- ifelse(val_data$event == 1, val_data$time, Inf)
dval <- xgb.DMatrix(
    data = val_x,
    label_lower_bound = val_y_lower_bound,
    label_upper_bound = val_y_upper_bound
)
```

## Hyperparameter Tuning

Since there is no

```{r}
# Ref Barnwal, A., Cho ,Hyunsu, & and Hocking, T. (2022). Survival Regression with Accelerated Failure Time Model in XGBoost. Journal of Computational and Graphical Statistics, 31(4), 1292â€“1302. https://doi.org/10.1080/10618600.2022.2067548
param_set <- ParamSet$new(params = list(
    learning_rate = p_dbl(
        lower = log10(0.001),
        upper = log10(1.0),
        trafo = function(x) 10^x
    ),
    max_depth = p_int(
        lower = 2, 
        upper = 10
    ),
    min_child_weight = p_dbl(
        lower = log10(0.001),
        upper = log10(100.0),
        trafo = function(x) 10^x
    ),
    reg_alpha = p_dbl(
        lower = log10(0.001),
        upper = log10(100.0),
        trafo = function(x) 10^x
    ),
    reg_lambda = p_dbl(
        lower = log10(0.001),
        upper = log10(100.0),
        trafo = function(x) 10^x
    ),
    aft_loss_distribution_scale = p_dbl(
        lower = 0.5,
        upper = 2.0
    )
))
```

```{r}
tune_xgb <- function(params_trial) {
    model <- xgb.train(
        params = c(
            list(
                objective = "survival:aft",
                eval_metric = "aft-nloglik",
                aft_loss_distribution = "normal"
            ),
            params_trial
        ),
        data = dtrain,
        nrounds = 1000,
        early_stopping_rounds = 10,
        watchlist = list(train = dtrain, val = dval),
        verbose = 0
    )
    pred <- predict(model, dval)
    pred <- -pred
    # return(list(score = min(model$evaluation_log$val_aft_nloglik)))
    return(list(score = concordance.index(pred, val_data$time, val_data$event)$c.index))
}
```

```{r title = "Tuning XGBoost hyperparameters"}
tuning_results <- pmclapply(1:n_trials, function(i) {
    params_trial <- generate_design_random(param_set, n = 1)$data
    params_trial <- param_set$trafo(params_trial)
    score <- tune_xgb(params_trial)
    return(data.frame(trial = i, score = score$score, params = params_trial))
}, title = "Tuning XGBoost hyperparameters")
```

```{r echo = FALSE}
save(tuning_results,
    file = get_data_path("xgb_tuning_results", adjust_type_str, impute_type_str, include_statin, model = "xgb")
)
```

```{r echo = FALSE}
load(get_data_path("xgb_tuning_results", adjust_type_str, impute_type_str, include_statin, model = "xgb"))
```

```{r}
tuning_results_valid <- list()
error_indices <- c()
for (i in seq_along(tuning_results)) {
    if (!inherits(tuning_results[[i]], "try-error")) {
        tuning_results_valid[[length(tuning_results_valid) + 1]] <- tuning_results[[i]]
    } else {
        error_indices <- c(error_indices, i)
        cat("Error in trial", i, ":", as.character(tuning_results[[i]]), "\n")
    }
}
```

```{r}
tuning_results <- bind_rows(tuning_results_valid)  # convert list of lists to a data frame
tuning_results_best <- tuning_results[which.max(tuning_results$score), ]


model_params <- list(
    learning_rate = tuning_results_best$params.learning_rate,
    max_depth = tuning_results_best$params.max_depth,
    min_child_weight = tuning_results_best$params.min_child_weight,
    reg_alpha = tuning_results_best$params.reg_alpha,
    reg_lambda = tuning_results_best$params.reg_lambda,
    aft_loss_distribution_scale = tuning_results_best$params.aft_loss_distribution_scale
)
print("Best hyperparameters:")
print(model_params)
```

## Variable Selection

```{r label = "Variable Selection-1"}
# * As mentioned in the paper, we use AFT model instead of Cox model.
xgb_var_select <- xgb.train(
    params = c(
        list(
            objective = "survival:aft",
            eval_metric = "aft-nloglik",
            aft_loss_distribution = "normal"
        ),
        model_params
    ),
    data = dtrain,
    nrounds = 1000,
    early_stopping_rounds = 10,
    watchlist = list(train = dtrain, val = dval)
)
```

```{r echo = FALSE}
save(xgb_var_select,
    file = get_data_path("xgb_var_select_model", adjust_type_str, impute_type_str, include_statin, model = "xgb")
)
```

```{r echo = FALSE}
load(get_data_path("xgb_var_select_model", adjust_type_str, impute_type_str, include_statin, model = "xgb"))
```

```{r label = "Variable Selection-2"}
# Sort descendingly using gain
xgb_importance <- xgb.importance(model = xgb_var_select)
# Other attributes: Gain, Cover, Frequency
vars_ranked <- xgb_importance$Feature
```

## Cross Validation to Select the Best Number of Features

```{r label = "Cross Validation to Select the Best Number of Features"}
# * xgb.cv is not available for AFT model.
set.seed(1234)
folds <- createFolds(data$event, k = n_folds)

cv_errors <- pmclapply(seq(1, length(vars_ranked), by = 1), function(num_vars) {
    selected_vars <- vars_ranked[1:num_vars]
    print(paste0("Selecting ", num_vars, " variables"))
    fold_errors <- sapply(folds, function(fold_idx) {
        # * We take all training data and validation data and then split them into folds.
        train_x_fold <- as.matrix(total_x[-fold_idx, selected_vars, drop = FALSE])
        train_y_lower_fold <- total_y_lower_bound[-fold_idx]
        train_y_upper_fold <- total_y_upper_bound[-fold_idx]

        val_x_fold <- as.matrix(total_x[fold_idx, selected_vars, drop = FALSE])
        val_y_lower_fold <- total_y_lower_bound[fold_idx]
        val_y_upper_fold <- total_y_upper_bound[fold_idx]
        val_y_fold <- total_y[fold_idx, ]  # for C-index calculation

        dtrain_fold <- xgb.DMatrix(
            data = train_x_fold,
            label_lower_bound = train_y_lower_fold,
            # label_upper_bound = train_y_upper_fold
            label_upper_bound = train_y_lower_fold
        )

        dval_fold <- xgb.DMatrix(
            data = val_x_fold,
            label_lower_bound = val_y_lower_fold,
            # label_upper_bound = val_y_upper_fold
            label_upper_bound = val_y_lower_fold
        )

        model <- xgb.train(
            params = c(
                list(
                    objective = "survival:aft",
                    eval_metric = "aft-nloglik",
                    aft_loss_distribution = "normal"
                ),
                model_params
            ),
            data = dtrain_fold,
            nrounds = 1000,
            early_stopping_rounds = 10,
            watchlist = list(train = dtrain_fold, val = dval_fold),
            verbose = 0
        )
        # * It outputs the estimated survival time. We need to convert it to risk.
        pred <- predict(model, dval_fold)
        pred <- -pred

        # Use C-index to measure the performance of the model
        1 - concordance.index(pred, val_y_fold$time, val_y_fold$event)$c.index
    })
    print(mean(fold_errors))
    mean(fold_errors)
}, title = "Cross Validation to Select the Best Number of Features")
```

```{r include = FALSE}
best_num_vars <- which.min(cv_errors)
vars_selected <- vars_ranked[1:best_num_vars]
```

```{r echo = FALSE}
save(cv_errors, vars_selected,
    file = get_data_path("xgb_var_select_name", adjust_type_str, impute_type_str, include_statin, model = "xgb")
)
```

```{r echo = FALSE}
load(get_data_path("xgb_var_select_name", adjust_type_str, impute_type_str, include_statin, model = "xgb"))
```

```{r}
cv_errors <- as.numeric(cv_errors)
plot(1:length(cv_errors), cv_errors)
best_num_vars <- which.min(cv_errors)
vars_selected <- vars_ranked[1:best_num_vars]
```

```{r}
print(paste0("The best number of features to retain is ", best_num_vars))
```

## Model Fitting

```{r label = "Model Fitting"}
dtrain_selected <- xgb.DMatrix(
    data = train_x[, vars_selected],
    label_lower_bound = train_y_lower_bound,
    label_upper_bound = train_y_upper_bound
)
dval_selected <- xgb.DMatrix(
    data = val_x[, vars_selected],
    label_lower_bound = val_y_lower_bound,
    label_upper_bound = val_y_upper_bound
)
xgb_model <- xgb.train(
    params = c(
        list(
            objective = "survival:aft",
            eval_metric = "aft-nloglik",
            aft_loss_distribution = "normal"
        ),
        model_params
    ),
    data = dtrain_selected,
    nrounds = 1000,
    early_stopping_rounds = 10,
    watchlist = list(train = dtrain_selected, val = dval_selected)
)
```

```{r}
dtrain_full <- xgb.DMatrix(
    data = train_x,
    label_lower_bound = train_y_lower_bound,
    label_upper_bound = train_y_upper_bound
)
# We also fit the full model
xgb_model_full <- xgb.train(
    params = c(
        list(
            objective = "survival:aft",
            eval_metric = "aft-nloglik",
            aft_loss_distribution = "normal"
        ),
        model_params
    ),
    data = dtrain_full,
    nrounds = 1000,
    early_stopping_rounds = 10,
    watchlist = list(train = dtrain_full, val = dval)
)
```

```{r echo = FALSE}
save(xgb_model, xgb_model_full,
    file = get_data_path("xgb_model", adjust_type_str, impute_type_str, include_statin, model = "xgb")
)
```

```{r echo = FALSE}
load(get_data_path("xgb_model", adjust_type_str, impute_type_str, include_statin, model = "xgb"))
```


```{r}
# SHAP?
```

The variable importance analysis across four different modeling approaches revealed the incremental predictive ability of several HRV indices beyond traditional cardiovascular risk factors. Among nonlinear HRV measures, Approximate Entropy (ApEn) consistently demonstrated its prognostic value, being selected by both RSF (VIMP=0.015) and XGBoost (gain=0.030) models, while Katz's Fractal Dimension was also identified as an important predictor by XGBoost (gain=0.015). For linear HRV indices, the HRV Triangular Index showed substantial predictive power in the Stepwise model (HR=1.103), and the 20th Percentile of RR Intervals was selected by both Stepwise (HR=0.871) and XGBoost (gain=0.026) models. The Contribution of Decelerations to Long-term HRV, a measure of heart rate deceleration patterns, was also identified as an important predictor by XGBoost (gain=0.018). Notably, while traditional cardiovascular risk factors maintained their strong prognostic value across all models - particularly age (consistently the top predictor), sex, hypertension treatment, and BMI - the selection of multiple HRV indices by different modeling approaches suggests their complementary role in cardiovascular risk prediction.